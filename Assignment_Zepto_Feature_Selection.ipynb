{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Aj5N4a7cMNEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce57833-e21f-4d0b-8072-621b34ef47c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "L5Xwbqd_Mxzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "file_path = '/content/drive/MyDrive/IIT BHU - DS Assessment dataset/data.csv'\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "1iNm46PkMzii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Handle Missing Values\n",
        "# Fill missing numerical values with the median\n",
        "numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].median())\n",
        "\n",
        "# Fill missing categorical values with the most frequent value\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])"
      ],
      "metadata": {
        "id": "MEEYvnncVfsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Feature Extraction\n",
        "# Example features: Length of search term, whether the search term is popular (based on frequency)\n",
        "data['search_term_length'] = data['search_term'].apply(len)\n",
        "data['is_popular_search_term'] = data['search_term'].isin(data['search_term'].value_counts().head(20).index).astype(int)"
      ],
      "metadata": {
        "id": "_CHRA9nwViYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Encode Categorical Variables\n",
        "# Using OneHotEncoder for categorical columns\n",
        "# Remove the target variable from numerical and categorical columns\n",
        "target = 'is_clicked' # Define the target variable here\n",
        "numerical_cols = numerical_cols.drop(target, errors='ignore')  # Ignore if target is not in the list\n",
        "categorical_cols = categorical_cols.drop(target, errors='ignore')  # Ignore if target is not in the list\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ])"
      ],
      "metadata": {
        "id": "ZjOpD8cFVk27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Target Variable\n",
        "X = data.drop(columns=[target])\n",
        "y = data[target]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the preprocessing pipeline\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)\n"
      ],
      "metadata": {
        "id": "a6tR2yq3VnO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Build a Simple Prediction Model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Feature Importance (for linear models)\n",
        "importance = model.coef_\n",
        "# Adjust feature names after removing target from numerical and categorical columns\n",
        "feature_names = numerical_cols.tolist() + preprocessor.transformers_[1][1].get_feature_names_out(categorical_cols).tolist()\n",
        "feature_importance = pd.Series(importance, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igYc01L8mI63",
        "outputId": "b4648760-b47c-4a56-e3e8-249e2055314f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.12659483881126682\n",
            "Feature Importance:\n",
            "query_type_head                                                                                          27.737006\n",
            "query_type_tail                                                                                          27.486191\n",
            "query_type_5.342857142857146                                                                             18.541543\n",
            "query_type_54.01                                                                                         18.429675\n",
            "predicted_category_name_{'Cleaning Essentials': 1, 'Electricals & Accessories': 0.5, 'Zepto Cafe': 2}    17.673990\n",
            "                                                                                                           ...    \n",
            "query_type_82.537090909091                                                                              -17.933973\n",
            "product_variant_id_Breakfast & Sauces                                                                   -55.223197\n",
            "city_id_Honey & Spreads                                                                                 -55.223197\n",
            "predicted_category_name_0                                                                               -55.223197\n",
            "query_type_66.51890909090915                                                                            -78.733301\n",
            "Length: 62070, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Retrieve column names from the transformers\n",
        "num_cols_transformed = numerical_cols\n",
        "# Access OneHotEncoder using 'onehot'\n",
        "cat_cols_transformed = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
        "\n",
        "all_cols_transformed = np.concatenate([num_cols_transformed, cat_cols_transformed])\n",
        "\n",
        "print(len(num_cols_transformed))\n",
        "print(len(cat_cols_transformed))\n",
        "print(len(all_cols_transformed))\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "W6pQqPlknN7d",
        "outputId": "68e17c13-0cae-444b-b0c9-486be72a9971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Retrieve column names from the transformers\\nnum_cols_transformed = numerical_cols\\n# Access OneHotEncoder using 'onehot'\\ncat_cols_transformed = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\\n\\nall_cols_transformed = np.concatenate([num_cols_transformed, cat_cols_transformed])\\n\\nprint(len(num_cols_transformed))\\nprint(len(cat_cols_transformed))\\nprint(len(all_cols_transformed))\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Option 1: Remove rows with any missing values\n",
        "data_dropped = data.dropna()\n",
        "\n",
        "# Option 2: Fill missing values (imputation)\n",
        "# For numerical columns, fill with the median\n",
        "numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "data_filled = data.copy()\n",
        "data_filled[numerical_cols] = data_filled[numerical_cols].fillna(data_filled[numerical_cols].median())\n",
        "\n",
        "# For categorical columns, fill with the most frequent value\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "data_filled[categorical_cols] = data_filled[categorical_cols].fillna(data_filled[categorical_cols].mode().iloc[0])\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "data_dropped.to_csv('/content/drive/MyDrive/IIT BHU - DS Assessment dataset/data_dropped.csv', index=False)\n",
        "data_filled.to_csv('/content/drive/MyDrive/IIT BHU - DS Assessment dataset/data.csv', index=False)\n",
        "\n",
        "print(\"Data with rows containing any missing values removed:\")\n",
        "print(data_dropped.head())\n",
        "\n",
        "print(\"\\nData with missing values filled:\")\n",
        "print(data_filled.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "9n5mHHF7tgyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d30dd98-2ac2-4db4-d2ba-269ea106817e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data with rows containing any missing values removed:\n",
            "           search_term                    product_variant_id  \\\n",
            "0    akshayakalpa milk  c442ad9b-09b6-4505-a17d-7d2b3cceee0e   \n",
            "1            ice cubes  33e1c372-8f6b-4312-b4df-93911f4c1caf   \n",
            "2              protien  8ed8801e-f3bd-46ed-a212-24dcaa33d937   \n",
            "3                 comb  b16224dd-5b0b-4e39-91d0-b52e563c70c2   \n",
            "4  nail polish remover  25efac15-9a86-4da1-ab36-94e32bc7ecd9   \n",
            "\n",
            "                                city_id query_type  is_clicked  total_clicks  \\\n",
            "0  ee66dc2a-aded-4445-a7b2-1ad63715725c       head         0.0           0.0   \n",
            "1  7e926d2f-adad-4e5a-956f-f07fffa54164       head         0.0           0.0   \n",
            "2  078d5e32-627a-4907-8df8-4360bc7c06da       head         1.0          29.0   \n",
            "3  4f30407c-6a3c-4a4e-8a3d-652217d4b6cb       head         0.0           0.0   \n",
            "4  4f30407c-6a3c-4a4e-8a3d-652217d4b6cb       tail         0.0           0.0   \n",
            "\n",
            "   session_views  query_products_clicks_last_30_days  CTR_last_30_days  \\\n",
            "0            0.0                                   0          0.000000   \n",
            "1           57.0                                   0          0.000000   \n",
            "2          545.0                                   9          0.040179   \n",
            "3           13.0                                   0          0.000000   \n",
            "4            1.0                                   0          0.000000   \n",
            "\n",
            "   CTR_last_7_days  ...  savings_with_pass  ad_revenue  total_unique_orders  \\\n",
            "0              0.0  ...           2.718400    0.000000                155.0   \n",
            "1              0.0  ...           5.009091    0.000000                 14.0   \n",
            "2              0.0  ...           3.239500   18.622949               1469.0   \n",
            "3              0.0  ...           5.118421    0.000000              14567.0   \n",
            "4              0.0  ...           4.915103   20.365139               1043.0   \n",
            "\n",
            "   product_atcs_30_days product_atcs_plt_30_days  \\\n",
            "0                 376.0                 552508.0   \n",
            "1                  41.0                   3264.0   \n",
            "2                3335.0                  55215.0   \n",
            "3               28849.0                 216304.0   \n",
            "4                2446.0                  14527.0   \n",
            "\n",
            "  total_unique_orders_plt_30_days  product_ctr_city_30_days  \\\n",
            "0                        167494.0                  0.054913   \n",
            "1                          1313.0                  0.003239   \n",
            "2                         24085.0                  0.061085   \n",
            "3                        121334.0                  0.120340   \n",
            "4                          6418.0                  0.052714   \n",
            "\n",
            "  query_product_similarity search_term_length is_popular_search_term  \n",
            "0                 0.384844                 17                      0  \n",
            "1                 0.324977                  9                      0  \n",
            "2                 0.220603                  7                      0  \n",
            "3                 0.061622                  4                      1  \n",
            "4                 0.390254                 19                      0  \n",
            "\n",
            "[5 rows x 33 columns]\n",
            "\n",
            "Data with missing values filled:\n",
            "           search_term                    product_variant_id  \\\n",
            "0    akshayakalpa milk  c442ad9b-09b6-4505-a17d-7d2b3cceee0e   \n",
            "1            ice cubes  33e1c372-8f6b-4312-b4df-93911f4c1caf   \n",
            "2              protien  8ed8801e-f3bd-46ed-a212-24dcaa33d937   \n",
            "3                 comb  b16224dd-5b0b-4e39-91d0-b52e563c70c2   \n",
            "4  nail polish remover  25efac15-9a86-4da1-ab36-94e32bc7ecd9   \n",
            "\n",
            "                                city_id query_type  is_clicked  total_clicks  \\\n",
            "0  ee66dc2a-aded-4445-a7b2-1ad63715725c       head         0.0           0.0   \n",
            "1  7e926d2f-adad-4e5a-956f-f07fffa54164       head         0.0           0.0   \n",
            "2  078d5e32-627a-4907-8df8-4360bc7c06da       head         1.0          29.0   \n",
            "3  4f30407c-6a3c-4a4e-8a3d-652217d4b6cb       head         0.0           0.0   \n",
            "4  4f30407c-6a3c-4a4e-8a3d-652217d4b6cb       tail         0.0           0.0   \n",
            "\n",
            "   session_views  query_products_clicks_last_30_days  CTR_last_30_days  \\\n",
            "0            0.0                                   0          0.000000   \n",
            "1           57.0                                   0          0.000000   \n",
            "2          545.0                                   9          0.040179   \n",
            "3           13.0                                   0          0.000000   \n",
            "4            1.0                                   0          0.000000   \n",
            "\n",
            "   CTR_last_7_days  ...  savings_with_pass  ad_revenue  total_unique_orders  \\\n",
            "0              0.0  ...           2.718400    0.000000                155.0   \n",
            "1              0.0  ...           5.009091    0.000000                 14.0   \n",
            "2              0.0  ...           3.239500   18.622949               1469.0   \n",
            "3              0.0  ...           5.118421    0.000000              14567.0   \n",
            "4              0.0  ...           4.915103   20.365139               1043.0   \n",
            "\n",
            "   product_atcs_30_days product_atcs_plt_30_days  \\\n",
            "0                 376.0                 552508.0   \n",
            "1                  41.0                   3264.0   \n",
            "2                3335.0                  55215.0   \n",
            "3               28849.0                 216304.0   \n",
            "4                2446.0                  14527.0   \n",
            "\n",
            "  total_unique_orders_plt_30_days  product_ctr_city_30_days  \\\n",
            "0                        167494.0                  0.054913   \n",
            "1                          1313.0                  0.003239   \n",
            "2                         24085.0                  0.061085   \n",
            "3                        121334.0                  0.120340   \n",
            "4                          6418.0                  0.052714   \n",
            "\n",
            "  query_product_similarity search_term_length is_popular_search_term  \n",
            "0                 0.384844                 17                      0  \n",
            "1                 0.324977                  9                      0  \n",
            "2                 0.220603                  7                      0  \n",
            "3                 0.061622                  4                      1  \n",
            "4                 0.390254                 19                      0  \n",
            "\n",
            "[5 rows x 33 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define chunk size\n",
        "chunk_size = 10000\n",
        "\n",
        "# Load the first chunk of data\n",
        "file_path = '/content/drive/MyDrive/IIT BHU - DS Assessment dataset/data_dropped.csv'\n",
        "chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
        "\n",
        "# Identify numerical and categorical columns from the first chunk\n",
        "first_chunk = next(chunks)\n",
        "numerical_cols = first_chunk.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = first_chunk.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define preprocessing pipelines\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Process the first chunk\n",
        "first_chunk_preprocessed = preprocessor.fit_transform(first_chunk)\n",
        "\n",
        "# Initialize a list to collect processed chunks\n",
        "processed_chunks = [pd.DataFrame(first_chunk_preprocessed, columns=np.concatenate([numerical_cols, preprocessor.transformers_[1][1].get_feature_names_out(categorical_cols)]))]\n",
        "\n",
        "# Process remaining chunks\n",
        "for chunk in chunks:\n",
        "    chunk_preprocessed = preprocessor.transform(chunk)\n",
        "    processed_chunk = pd.DataFrame(chunk_preprocessed, columns=np.concatenate([numerical_cols, preprocessor.transformers_[1][1].get_feature_names_out(categorical_cols)]))\n",
        "    processed_chunks.append(processed_chunk)\n",
        "\n",
        "# Concatenate all processed chunks\n",
        "data_cleaned = pd.concat(processed_chunks, ignore_index=True)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "cleaned_file_path = '/content/drive/MyDrive/IIT BHU - DS Assessment dataset/data_dropped_cleaned.csv'\n",
        "data_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "# Correlation Analysis\n",
        "correlation_matrix = data_cleaned.corr()\n",
        "\n",
        "# Plot the correlation matrix\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of Cleaned Data')\n",
        "plt.show()\n",
        "\n",
        "print(\"Cleaned and Preprocessed Data:\")\n",
        "print(data_cleaned.head())\n"
      ],
      "metadata": {
        "id": "o_i8MfsasvAj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}